"""
Simple NotebookLM - Document Q&A with S3 Vectors and Bedrock Claude

A Streamlit application for uploading documents and asking questions using RAG.
"""

import streamlit as st
from config import Config
from utils import (
    DocumentProcessor,
    TextSplitter,
    EmbeddingsGenerator,
    S3VectorStore,
    RAGEngine
)

# Page configuration
st.set_page_config(
    page_title="Simple NotebookLM",
    page_icon="ğŸ“š",
    layout="wide"
)


def initialize_session_state():
    """Initialize Streamlit session state variables"""
    if 'document_processed' not in st.session_state:
        st.session_state.document_processed = False
    if 'document_name' not in st.session_state:
        st.session_state.document_name = None
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []


def validate_config():
    """Validate configuration and show errors if needed"""
    try:
        Config.validate()
        return True
    except ValueError as e:
        st.error(f"âš ï¸ Configuration Error: {str(e)}")
        st.info("Please set up your `.env` file with required AWS credentials and S3 Vector configuration.")
        return False


def process_document(uploaded_file):
    """Process uploaded document through the RAG pipeline"""
    try:
        # Extract text from document
        with st.spinner("ğŸ“„ ë¬¸ì„œì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì¤‘..."):
            file_bytes = uploaded_file.read()
            filename = uploaded_file.name

            processor = DocumentProcessor()
            documents = processor.process_document(file_bytes, filename)

            st.success(f"âœ… {len(documents)}ê°œ í˜ì´ì§€/ì„¹ì…˜ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ")

        # Split into chunks
        with st.spinner("âœ‚ï¸ í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ëŠ” ì¤‘..."):
            splitter = TextSplitter()
            chunks = splitter.split_documents(documents)

            st.success(f"âœ… {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

        # Generate embeddings
        with st.spinner("ğŸ§® ì„ë² ë”© ë²¡í„° ìƒì„± ì¤‘... (ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)"):
            embeddings_gen = EmbeddingsGenerator()
            chunk_texts = [chunk['content'] for chunk in chunks]
            embeddings = embeddings_gen.generate_embeddings_batch(chunk_texts)

            valid_count = sum(1 for e in embeddings if e is not None)
            st.success(f"âœ… {valid_count}ê°œ ì„ë² ë”© ë²¡í„° ìƒì„± ì™„ë£Œ")

        # Store in S3 Vectors
        with st.spinner("ğŸ’¾ S3 Vectorsì— ì €ì¥ ì¤‘..."):
            vector_store = S3VectorStore()
            result = vector_store.put_vectors(chunks, embeddings)

            st.success(f"âœ… {result['total_stored']}ê°œ ë²¡í„° ì €ì¥ ì™„ë£Œ ({result['batches']}ê°œ ë°°ì¹˜)")

        st.session_state.document_processed = True
        st.session_state.document_name = filename

        return True

    except Exception as e:
        st.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False


def display_answer(result):
    """Display answer with sources in a formatted way"""
    # Display answer
    st.markdown("### ğŸ’¬ ë‹µë³€")
    st.markdown(result['answer'])

    # Display sources if available
    if result.get('sources'):
        st.markdown("---")
        st.markdown("### ğŸ“ ì¶œì²˜")

        for i, source in enumerate(result['sources'], 1):
            with st.expander(f"ì¶œì²˜ {i}: {source['document']} (í˜ì´ì§€ {source['page']}, ìœ ì‚¬ë„: {source['similarity']:.2%})"):
                st.text(source['preview'])

    # Display retrieval stats
    if result.get('retrieval_stats'):
        stats = result['retrieval_stats']
        st.markdown("---")
        st.caption(
            f"ğŸ“Š ê²€ìƒ‰ í†µê³„: {stats['total_relevant']}/{stats['total_retrieved']}ê°œ ê´€ë ¨ ì²­í¬ ë°œê²¬ "
            f"(ì„ê³„ê°’: {stats['similarity_threshold']})"
        )


def main():
    """Main application"""
    st.title("ğŸ“š Simple NotebookLM")
    st.markdown("**AWS S3 Vectorsì™€ Bedrock Claudeë¥¼ í™œìš©í•œ ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ**")

    initialize_session_state()

    # Validate configuration
    if not validate_config():
        st.stop()

    # Sidebar - Document Upload
    with st.sidebar:
        st.header("ğŸ“¤ ë¬¸ì„œ ì—…ë¡œë“œ")

        uploaded_file = st.file_uploader(
            "PDF, DOCX, TXT íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=['pdf', 'docx', 'txt'],
            help="ì—…ë¡œë“œëœ ë¬¸ì„œëŠ” ìë™ìœ¼ë¡œ ë¶„ì„ë˜ì–´ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆë„ë¡ ì²˜ë¦¬ë©ë‹ˆë‹¤."
        )

        if uploaded_file:
            if st.button("ğŸ“„ ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘", type="primary", use_container_width=True):
                process_document(uploaded_file)

        # Show current document status
        if st.session_state.document_processed:
            st.success(f"âœ… í˜„ì¬ ë¬¸ì„œ: {st.session_state.document_name}")
        else:
            st.info("â³ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”")

        # Configuration display
        st.markdown("---")
        st.markdown("### âš™ï¸ ì„¤ì •")
        st.caption(f"ë¦¬ì „: {Config.AWS_REGION}")
        st.caption(f"ì²­í¬ í¬ê¸°: {Config.CHUNK_SIZE}")
        st.caption(f"ì¤‘ë³µ: {Config.CHUNK_OVERLAP}")
        st.caption(f"Top-K: {Config.TOP_K_RESULTS}")
        st.caption(f"ìœ ì‚¬ë„ ì„ê³„ê°’: {Config.SIMILARITY_THRESHOLD}")

    # Main area - Q&A Interface
    if st.session_state.document_processed:
        st.markdown("### ğŸ’­ ì§ˆë¬¸í•˜ê¸°")
        st.markdown(f"**{st.session_state.document_name}**ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!")

        # Question input
        question = st.text_input(
            "ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”:",
            placeholder="ì˜ˆ: ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            key="question_input"
        )

        col1, col2 = st.columns([1, 5])
        with col1:
            ask_button = st.button("ğŸ” ì§ˆë¬¸í•˜ê¸°", type="primary")

        if ask_button and question:
            with st.spinner("ğŸ¤” ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘..."):
                try:
                    rag_engine = RAGEngine()
                    result = rag_engine.ask(question)

                    # Add to chat history
                    st.session_state.chat_history.append({
                        'question': question,
                        'result': result
                    })

                    # Display answer
                    display_answer(result)

                except Exception as e:
                    st.error(f"âŒ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

        # Display chat history
        if st.session_state.chat_history:
            st.markdown("---")
            st.markdown("### ğŸ“œ ëŒ€í™” ê¸°ë¡")

            for i, chat in enumerate(reversed(st.session_state.chat_history), 1):
                with st.expander(f"ì§ˆë¬¸ {len(st.session_state.chat_history) - i + 1}: {chat['question'][:50]}..."):
                    st.markdown(f"**ì§ˆë¬¸:** {chat['question']}")
                    st.markdown(f"**ë‹µë³€:** {chat['result']['answer']}")

                    if chat['result'].get('sources'):
                        st.caption(f"ì¶œì²˜: {len(chat['result']['sources'])}ê°œ")

    else:
        st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")

        # Show instructions
        st.markdown("### ğŸ“– ì‚¬ìš© ë°©ë²•")
        st.markdown("""
        1. **ë¬¸ì„œ ì—…ë¡œë“œ**: ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ PDF, DOCX, TXT íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”
        2. **ë¬¸ì„œ ì²˜ë¦¬**: "ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘" ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì„¸ìš”
        3. **ì§ˆë¬¸í•˜ê¸°**: ì²˜ë¦¬ê°€ ì™„ë£Œë˜ë©´ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
        4. **ë‹µë³€ í™•ì¸**: AIê°€ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ê³  ì¶œì²˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤

        **ê¸°ìˆ  ìŠ¤íƒ:**
        - ğŸ”¤ **í…ìŠ¤íŠ¸ ì¶”ì¶œ**: PyPDF2, python-docx
        - âœ‚ï¸ **ì²­í¬ ë¶„í• **: LangChain RecursiveCharacterTextSplitter
        - ğŸ§® **ì„ë² ë”©**: AWS Bedrock Titan Text Embeddings V2 (1024ì°¨ì›)
        - ğŸ’¾ **ë²¡í„° ì €ì¥**: AWS S3 Vectors (Native Vector Storage)
        - ğŸ¤– **ë‹µë³€ ìƒì„±**: AWS Bedrock Claude 3 Sonnet
        """)


if __name__ == "__main__":
    main()
